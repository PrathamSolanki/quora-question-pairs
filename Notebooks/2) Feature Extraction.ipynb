{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../Data/train.csv')\n",
    "#train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../Data/test.csv')\n",
    "#test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Aim is to Extract features that can differentiate whether the pair of questions are duplicate or not\n",
    "## 1) Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "punctuations = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 2) Word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_words(question):\n",
    "    #question = question.decode('utf-8').replace(u'\\u014c\\u0106\\u014d','-')\n",
    "    return word_tokenize(question.lower())\n",
    "\n",
    "train_df[\"words_ques1\"] = train_df['question1'].apply(lambda x: get_words(str(x)))\n",
    "train_df[\"words_ques2\"] = train_df['question2'].apply(lambda x: get_words(str(x)))\n",
    "\n",
    "#test_df[\"words_ques1\"] = test_df['question1'].apply(lambda x: get_words(str(x)))\n",
    "#test_df[\"words_ques2\"] = test_df['question2'].apply(lambda x: get_words(str(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 3) Unigrams (removing stop words and punctuations, selecting only unique words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_unigrams(words):\n",
    "    unigrams = [] \n",
    "    for word in words:\n",
    "        if word not in unigrams and word not in eng_stopwords and word not in punctuations:\n",
    "            unigrams.append(word)\n",
    "            \n",
    "    return unigrams\n",
    "\n",
    "train_df[\"unigrams_ques1\"] = train_df['words_ques1'].apply(lambda x: get_unigrams(x))\n",
    "train_df[\"unigrams_ques2\"] = train_df['words_ques2'].apply(lambda x: get_unigrams(x))\n",
    "\n",
    "#test_df[\"unigrams_ques1\"] = test_df['words_ques1'].apply(lambda x: get_unigrams(x))\n",
    "#test_df[\"unigrams_ques2\"] = test_df['words_ques2'].apply(lambda x: get_unigrams(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 4) Stemming the unigrams (Porter Stemmer Bugged in 3.2.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#from nltk.stem.porter import *\n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#def stem_words(words):\n",
    "#    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "#train_df[\"unigrams_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: stem_words(x))\n",
    "#train_df[\"unigrams_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: stem_words(x))\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stem_words(words):\n",
    "    return [stemmer.stem(word) for word in words]\n",
    "\n",
    "train_df[\"unigrams_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: stem_words(x))\n",
    "train_df[\"unigrams_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: stem_words(x))\n",
    "\n",
    "#test_df[\"unigrams_ques1\"] = test_df['unigrams_ques1'].apply(lambda x: stem_words(x))\n",
    "#test_df[\"unigrams_ques2\"] = test_df['unigrams_ques2'].apply(lambda x: stem_words(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 5) Common unigrams between the question pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "unigrams_common = []\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    unigrams_common.append(set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])))\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "train_df[\"unigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "train_df[\"unigrams_common\"] = unigrams_common\n",
    "\n",
    "#test_df[\"unigrams_common_count\"] = test_df.apply(lambda row: get_common_unigrams(row),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 6) Ratio of common unigrams to the total number of words between question pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"unigrams_common_count\"]) / max(len( row[\"unigrams_common\"] ),1)\n",
    "\n",
    "train_df[\"unigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n",
    "\n",
    "#test_df[\"unigrams_common_ratio\"] = test_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 7) Simliarly for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_bigrams(question):\n",
    "    return [i for i in ngrams(question, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"bigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"bigrams_ques1\"] = train_df[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "train_df[\"bigrams_ques2\"] = train_df[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "train_df[\"bigrams_common_count\"] = train_df.apply(lambda row: get_common_bigrams(row),axis=1)\n",
    "train_df[\"bigrams_common_ratio\"] = train_df.apply(lambda row: get_common_bigram_ratio(row), axis=1)\n",
    "\n",
    "#test_df[\"bigrams_ques1\"] = test_df[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "#test_df[\"bigrams_ques2\"] = test_df[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "#test_df[\"bigrams_common_count\"] = test_df.apply(lambda row: get_common_bigrams(row),axis=1)\n",
    "#test_df[\"bigrams_common_ratio\"] = test_df.apply(lambda row: get_common_bigram_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 8) And similiarly for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_trigrams(question):\n",
    "    return [i for i in ngrams(question, 3)]\n",
    "\n",
    "def get_common_trigrams(row):\n",
    "    return len( set(row[\"trigrams_ques1\"]).intersection(set(row[\"trigrams_ques2\"])) )\n",
    "\n",
    "def get_common_trigram_ratio(row):\n",
    "    return float(row[\"trigrams_common_count\"]) / max(len( set(row[\"trigrams_ques1\"]).union(set(row[\"trigrams_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"trigrams_ques1\"] = train_df[\"unigrams_ques1\"].apply(lambda x: get_trigrams(x))\n",
    "train_df[\"trigrams_ques2\"] = train_df[\"unigrams_ques2\"].apply(lambda x: get_trigrams(x)) \n",
    "train_df[\"trigrams_common_count\"] = train_df.apply(lambda row: get_common_trigrams(row),axis=1)\n",
    "train_df[\"trigrams_common_ratio\"] = train_df.apply(lambda row: get_common_trigram_ratio(row), axis=1)\n",
    "\n",
    "#test_df[\"trigrams_ques1\"] = test_df[\"unigrams_ques1\"].apply(lambda x: get_trigrams(x))\n",
    "#test_df[\"trigrams_ques2\"] = test_df[\"unigrams_ques2\"].apply(lambda x: get_trigrams(x)) \n",
    "#test_df[\"trigrams_common_count\"] = test_df.apply(lambda row: get_common_trigrams(row),axis=1)\n",
    "#test_df[\"trigrams_common_ratio\"] = test_df.apply(lambda row: get_common_trigram_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 9) Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b21c874b06cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                 pos == 'NNPS']\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nouns_ques1\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigrams_ques1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextract_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nouns_ques2\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unigrams_ques2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mextract_nouns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "def extract_nouns(words):\n",
    "    tagged_sent = pos_tag(words)\n",
    "    return [word for word,pos in tagged_sent if pos == 'NN' or \n",
    "                                                pos == 'NNS' or\n",
    "                                                pos == 'NNP' or\n",
    "                                                pos == 'NNPS']\n",
    "\n",
    "train_df[\"nouns_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: extract_nouns(x))\n",
    "train_df[\"nouns_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: extract_nouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_common_nouns(row):\n",
    "    return len( set(row[\"nouns_ques1\"]).intersection(set(row[\"nouns_ques2\"])) )\n",
    "\n",
    "train_df[\"nouns_common_count\"] = train_df.apply(lambda row: get_common_nouns(row),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_common_nouns_ratio(row):\n",
    "    return float(row[\"nouns_common_count\"]) / max(len( set(row[\"nouns_ques1\"]).union(set(row[\"nouns_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"nouns_common_ratio\"] = train_df.apply(lambda row: get_common_nouns_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 10) Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def extract_adjectives(words):\n",
    "    tagged_sent = pos_tag(words)\n",
    "    return [word for word,pos in tagged_sent if pos == 'JJ' or \n",
    "                                                pos == 'JJR' or\n",
    "                                                pos == 'JJS']\n",
    "\n",
    "train_df[\"adjectives_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: extract_adjectives(x))\n",
    "train_df[\"adjectives_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: extract_adjectives(x))\n",
    "\n",
    "\n",
    "def get_common_adjectives(row):\n",
    "    return len( set(row[\"adjectives_ques1\"]).intersection(set(row[\"adjectives_ques2\"])) )\n",
    "\n",
    "train_df[\"adjectives_common_count\"] = train_df.apply(lambda row: get_common_adjectives(row),axis=1)\n",
    "\n",
    "\n",
    "def get_common_adjectives_ratio(row):\n",
    "    return float(row[\"adjectives_common_count\"]) / max(len( set(row[\"adjectives_ques1\"]).union(set(row[\"adjectives_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"adjectives_common_ratio\"] = train_df.apply(lambda row: get_common_adjectives_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 11) Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_verbs(words):\n",
    "    tagged_sent = pos_tag(words)\n",
    "    return [word for word,pos in tagged_sent if pos == 'RB' or \n",
    "                                                pos == 'RBR' or\n",
    "                                                pos == 'RBS' or\n",
    "                                                pos == 'VB' or\n",
    "                                                pos == 'VBD' or\n",
    "                                                pos == 'VBG' or\n",
    "                                                pos == 'VBN' or\n",
    "                                                pos == 'VBP' or\n",
    "                                                pos == 'VBZ']\n",
    "\n",
    "train_df[\"verbs_ques1\"] = train_df['unigrams_ques1'].apply(lambda x: extract_verbs(x))\n",
    "train_df[\"verbs_ques2\"] = train_df['unigrams_ques2'].apply(lambda x: extract_verbs(x))\n",
    "\n",
    "\n",
    "def get_common_verbs(row):\n",
    "    return len( set(row[\"verbs_ques1\"]).intersection(set(row[\"verbs_ques2\"])) )\n",
    "\n",
    "train_df[\"verbs_common_count\"] = train_df.apply(lambda row: get_common_verbs(row),axis=1)\n",
    "\n",
    "\n",
    "def get_common_verbs_ratio(row):\n",
    "    return float(row[\"verbs_common_count\"]) / max(len( set(row[\"verbs_ques1\"]).union(set(row[\"verbs_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"verbs_common_ratio\"] = train_df.apply(lambda row: get_common_verbs_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 12) Word Match share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "train_df['word_match'] = train_df.apply(word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 13) tf-idf word match share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "train_qs = pd.Series(train_df['question1'].tolist() + train_df['question2'].tolist()).astype(str)\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "train_df['tfidf_train_word_match'] = train_df.apply(tfidf_word_match_share, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df.to_pickle('dataframe_toverify')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
